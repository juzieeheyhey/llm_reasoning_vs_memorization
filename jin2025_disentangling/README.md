# Reproduction: Disentangling Memory and Reasoning in LLMs (Jin et al., 2025)

This repository reproduces the core experiments from the paper: ðŸ“„ [Jin et al., 2025 â€” Disentangling Memory and Reasoning Ability in Large Language Models](https://arxiv.org/pdf/2411.13504)

> The codebase is **modified from the official implementation**:  
> https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning

## Core Methods Summary

The paper introduces a method to **disentangle memory (factual retrieval)** and **reasoning (inference)** in LLMs by:
- Generating Chain-of-Thought (CoT) reasoning traces for each question
- **Classifying each step** as either:
  - `[rag]`: requires factual knowledge
  - `[reason]`: involves logical reasoning
- Using LLMs to label steps via a reasoning-plan prompt
- Injecting **control tokens**:
  - `<memory>` for `[rag]` steps
  - `<reason>` for `[reason]` steps
- Fine-tuning decoder-only models with these structured CoT traces
- Evaluating on QA benchmarks: **StrategyQA**, **TruthfulQA**, and **CommonsenseQA**

## Directory structure
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ load_data/
â”‚   â”œâ”€â”€ data_agent.py # Generates the memory step and the reason step naturally without order 
â”‚   â”œâ”€â”€ data_agent_order.py # Generate the memory step and then generate the reason step
â”‚   â”œâ”€â”€ k_shot_dataset.py # Wraps datasets for k-shot in-context learning 
â”‚   â”œâ”€â”€ preprocess.py # Converts raw datasets into question/answer + CoT step formats
â”‚   â”œâ”€â”€ supervised_dataset.py # Prepares dataset and collator for standard supervised fine-tuning 
â”‚   â”œâ”€â”€ constant_len_dataset.py # Converts dataset into fixed-length token sequences for streaming training 
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ generation_utils.py # Custom decoding methods  
â”‚   â”œâ”€â”€ load_model.py # Wrapper to load base LLMs 
â”‚   â”œâ”€â”€ my_trainer.py # HF Trainer wrapper with evaluation, metric logging, saving 
â”‚   â””â”€â”€ sparse_models.py # Implements sparse attention models 
â”œâ”€â”€ eval.py # Runs model evaluation on test set 
â”œâ”€â”€ eval.sh # Shell script to run eval.py 
â”œâ”€â”€ train.py # Main training entrypoint: sets up tokenizer, dataset, model, trainer
â””â”€â”€ train.sh # Shell script to launch train.py
```