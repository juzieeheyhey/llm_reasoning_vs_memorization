{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/mj939/miniconda3/envs/reasonrag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json, random, os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "from bertviz import model_view\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import os, json, random, pickle,re\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from load_data.preprocess import GSMData, AquaData, StrategyQAData, StrategyQAData_Ours, CommonsenseQAData_Ours, TruthfulQAData_Ours\n",
    "from model.generation_utils import make_sparse_mask\n",
    "from model.load_model import MyAutoModelForCausalLM\n",
    "from model.peft_model import MyPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"...../checkpoints/meta-llama/Llama-3.1-8B-Instruct/commonsenseqa_agent/step_type=memory-3-4-efficient=lora+prompt-tuning-lr=2e-4-soft-prompt=True/checkpoint-1466\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint_path) \n",
    "prompt_text = json.load(open(\".../checkpoints/meta-llama/Llama-3.1-8B-Instruct/commonsenseqa_agent/step_type=memory-3-4-efficient=lora+prompt-tuning-lr=2e-4-soft-prompt=True/prompt_text.json\"))\n",
    "print(prompt_text)\n",
    "special_tokens_list = []\n",
    "for k in prompt_text:\n",
    "    tokens = prompt_text[k].split('>')\n",
    "    special_tokens_list += [tok+'>' for tok in tokens[:-1]]\n",
    "prompt_tokens = tokenizer.convert_tokens_to_ids(special_tokens_list)\n",
    "num_new_tokens = len(special_tokens_list)\n",
    "\n",
    "model = MyAutoModelForCausalLM.from_pretrained(n_tokens=num_new_tokens,\n",
    "            input_embedding_file=checkpoint_path + '/input_embeddings.pt',\n",
    "            output_embedding_file=checkpoint_path + '/output_embeddings.pt',\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            pretrained_model_name_or_path=checkpoint_path, #'meta-llama/Llama-2-7b-hf',\n",
    "            parameter_efficient_mode='lora+prompt-tuning',\n",
    "            cache_dir=None, torch_dtype=torch.float32, \n",
    "            device_map=\"cuda:0\", load_in_8bit=True,\n",
    "            offload_folder=\"offload\", offload_state_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Question: What room would you find many bookcases and is used for contemplation? Options: A.study B.house C.homw D.kitchen E.den \n",
    "<prefix_0><prefix_1><prefix_2><reason_0><reason_1><reason_2><reason_3>The question asks about a room where you would find many bookcases and which is used for contemplation. We need to identify the key features that describe this room.\n",
    "<prefix_0><prefix_1><prefix_2><reason_0><reason_1><reason_2><reason_3>The key features are \\\"many bookcases\\\" and \\\"used for contemplation.\\\" These features are important in determining the type of room being described.\n",
    "<prefix_0><prefix_1><prefix_2><memory_0><memory_1><memory_2><memory_3>A den is typically a small, intimate room used for reading, relaxation, or contemplation. It often contains many books and is designed as a quiet space. \n",
    "<prefix_0><prefix_1><prefix_2><reason_0><reason_1><reason_2><reason_3>Based on the key features and the knowledge that a den is used for contemplation and often contains many bookcases, the most suitable option is \\\"E. den.\\\" \n",
    "<prefix_0><prefix_1><prefix_2><answer_0><answer_1><answer_2><answer_3>The answer is: E\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, padding=True, return_tensors='pt').to('cuda:0')\n",
    "outputs = model(**inputs,  output_attentions=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "data = outputs.attentions[-1][0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(tokens)))\n",
    "cots = []\n",
    "start = 0\n",
    "end = 0\n",
    "for i in ids:\n",
    "    if tokens[i] == \"<prefix_0>\":\n",
    "        if start > 0:\n",
    "            cots.append((start, i))\n",
    "        start = i\n",
    "cots.append((start, i+1))\n",
    "print(cots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticklabels = [token.replace(\"Ġ\", \"-\") for token in tokens[cots[0][0]:cots[2][1]]]\n",
    "yticklabels = [token.replace(\"Ġ\", \"-\") for token in tokens[cots[0][0]:cots[2][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the color palette and adding bold labels for both x and y axes\n",
    "\n",
    "# Adjusting font scale for the heatmap and figure size\n",
    "sn.set(font_scale=0.49)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Sample attention data for plotting\n",
    "data = outputs.attentions[0][0][15].cpu().detach().numpy() # Example data for demonstration, replace with actual data when available\n",
    "\n",
    "# Plot the heatmap with a different color palette and bold axis labels\n",
    "hm = sn.heatmap(data=data[cots[0][0]:cots[2][1], cots[0][0]:cots[2][1]], \n",
    "                xticklabels=xticklabels, \n",
    "                yticklabels=yticklabels, \n",
    "                annot=False, )\n",
    "\n",
    "  # Choosing YlGnBu for a visually appealing color palette\n",
    "\n",
    "\n",
    "# Bold the tick labels on both x and y axes\n",
    "plt.xticks(fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.savefig(\"heatmap_llama3-8b.pdf\",bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Question: Was ship that recovered Apollo 13 named after a World War II battle?\\n  \n",
    "<prefix_0><prefix_1><prefix_2><memory_0><memory_1><memory_2><memory_3>The ship that recovered the Apollo 13 astronauts was the USS Iwo Jima. The USS Iwo Jima was named after the Battle of Iwo Jima, a pivotal battle in World War II\n",
    "<prefix_0><prefix_1><prefix_2><reason_0><reason_1><reason_2><reason_3>Since the ship was named after the Battle of Iwo Jima, it is reasonable to conclude that the ship was named after a World War II battle.\n",
    "<prefix_0><prefix_1><prefix_2><answer_0><answer_1><answer_2><answer_3>The answer is: True\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, padding=True, return_tensors='pt').to('cuda:0')\n",
    "outputs = model(**inputs,  output_attentions=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "data = outputs.attentions[-1][0][0].cpu().detach().numpy()\n",
    "\n",
    "ids = list(range(len(tokens)))\n",
    "cots = []\n",
    "start = 0\n",
    "end = 0\n",
    "for i in ids:\n",
    "    if tokens[i] == \"<prefix_0>\":\n",
    "        if start > 0:\n",
    "            cots.append((start, i))\n",
    "        start = i\n",
    "cots.append((start, i+1))\n",
    "print(cots)\n",
    "xticklabels = [token.replace(\"Ġ\", \"-\") for token in tokens[cots[0][0]:cots[2][1]]]\n",
    "yticklabels = [token.replace(\"Ġ\", \"-\") for token in tokens[cots[0][0]:cots[2][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the color palette and adding bold labels for both x and y axes\n",
    "\n",
    "# Adjusting font scale for the heatmap and figure size\n",
    "sn.set(font_scale=0.49)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Sample attention data for plotting\n",
    "data = outputs.attentions[0][0][15].cpu().detach().numpy() # Example data for demonstration, replace with actual data when available\n",
    "\n",
    "# Plot the heatmap with a different color palette and bold axis labels\n",
    "hm = sn.heatmap(data=data[cots[0][0]:cots[2][1], cots[0][0]:cots[2][1]], \n",
    "                xticklabels=xticklabels, \n",
    "                yticklabels=yticklabels, \n",
    "                annot=False, )\n",
    "\n",
    "  # Choosing YlGnBu for a visually appealing color palette\n",
    "\n",
    "\n",
    "# Bold the tick labels on both x and y axes\n",
    "plt.xticks(fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.savefig(\"heatmap_llama3-8b1.pdf\",bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasonrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
